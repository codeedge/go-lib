package sse

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"github.com/gin-contrib/sse"
	"github.com/gin-gonic/gin"
	"github.com/redis/go-redis/v9"
	"io"
	"log"
	"net/http"
	"runtime/debug"
	"strconv"
	"strings"
	"sync"
	"time"
)

type Service struct {
	nodeId                int                // èŠ‚ç‚¹id
	clients               map[string]*Client // å­˜å‚¨æ‰€æœ‰æœ¬åœ°å®¢æˆ·ç«¯è¿æ¥
	clientsSync           sync.RWMutex       // è¯»å†™é”
	rdb                   *redis.Client      // Rediså®¢æˆ·ç«¯
	ctx                   context.Context    // ä¸Šä¸‹æ–‡
	redisSessionKey       string             // ä¼šè¯å­˜å‚¨ è®°å½•ç”¨æˆ·-å®¢æˆ·ç«¯å¯¹åº”çš„èŠ‚ç‚¹id
	redisUserClientsKey   string             // ç”¨æˆ·-å®¢æˆ·ç«¯æ˜ å°„Key è®°å½•ç”¨æˆ·æ‰€æœ‰çš„å®¢æˆ·ç«¯idï¼Œå•æ¨æ—¶æŸ¥æ‰¾ç”¨æˆ·æ‰€æœ‰çš„å®¢æˆ·ç«¯è¿›è¡Œæ¨é€
	redisSessionSetKey    string             // èŠ‚ç‚¹å®¢æˆ·ç«¯é›†åˆkey ç”¨äºæ¸…ç†èŠ‚ç‚¹å®¢æˆ·ç«¯
	redisPubSubChannelKey string             // å‘å¸ƒè®¢é˜…key
	redisOfflineQueueKey  string             // ç¦»çº¿æ¶ˆæ¯é˜Ÿåˆ—Keyå‰ç¼€
	stopHeartbeat         chan struct{}      // åœæ­¢å¿ƒè·³ä¿¡å·
}

// Client è¡¨ç¤ºSSEå®¢æˆ·ç«¯è¿æ¥
type Client struct {
	userId      int64
	clientId    string
	c           *gin.Context
	messageChan chan string
}

// Message é›†ç¾¤èŠ‚ç‚¹é—´ä¼ è¾“çš„æ¶ˆæ¯æ ¼å¼
type Message struct {
	ClientId  string `json:"client_id"` // ç›®æ ‡å®¢æˆ·ç«¯Id
	NodeId    int    `json:"node_id"`   // æ¨¡æ¿å®¢æˆ·èŠ‚ç‚¹Id
	Type      int    `json:"type"`      // ç±»å‹
	Data      any    `json:"data"`      // æ•°æ®
	Timestamp int64  `json:"timestamp"` // æ—¶é—´æˆ³
}

var SSE *Service

const (
	redisSessionPrefix     = "sse:session:"       // ä¼šè¯å­˜å‚¨å‰ç¼€
	redisUserClientsPrefix = "sse:user:clients:"  // ç”¨æˆ·-å®¢æˆ·ç«¯æ˜ å°„Key
	redisSessionSet        = "sse:node:clients:"  // èŠ‚ç‚¹å®¢æˆ·ç«¯é›†åˆå‰ç¼€
	redisPubSubChannel     = "sse:cluster"        // é›†ç¾¤æ¶ˆæ¯é€šé“
	redisOfflineQueue      = "sse:offline-msg"    // ç¦»çº¿æ¶ˆæ¯é˜Ÿåˆ—
	sessionTTL             = 5 * 60 * time.Second // ä¼šè¯æœ‰æ•ˆæœŸ
	heartbeatInterval      = 60 * time.Second     // å¿ƒè·³é—´éš”
)

// New åˆ›å»ºSSEæœåŠ¡å®ä¾‹
func New(rds *redis.Client, nodeId int, redisPrefix string) *Service {
	SSE = &Service{
		nodeId:                nodeId,
		clients:               make(map[string]*Client),
		rdb:                   rds,
		ctx:                   context.Background(),
		redisSessionKey:       fmt.Sprintf("%s:%s", redisPrefix, redisSessionPrefix),
		redisUserClientsKey:   fmt.Sprintf("%s:%s", redisPrefix, redisUserClientsPrefix),
		redisSessionSetKey:    fmt.Sprintf("%s:%s%d", redisPrefix, redisSessionSet, nodeId),
		redisPubSubChannelKey: fmt.Sprintf("%s:%s", redisPrefix, redisPubSubChannel),
		redisOfflineQueueKey:  fmt.Sprintf("%s:%s", redisPrefix, redisOfflineQueue),
		stopHeartbeat:         make(chan struct{}),
	}

	// åˆå§‹åŒ–æ—¶æ¸…ç†redisæœ¬èŠ‚ç‚¹çš„ä¼šè¯è®°å½•
	SafeGoWithRestart("cleanupStaleSessions", SSE.cleanupStaleSessions, 3, 10*time.Second)
	// è®¢é˜…æœ¬èŠ‚ç‚¹ä¸“å±é¢‘é“
	SafeGoWithRestart("subscribeNodeChannel", SSE.subscribeNodeChannel, 3, 10*time.Second)
	// è®¢é˜…å…¨å±€æ§åˆ¶é¢‘é“ï¼ˆç”¨äºå¹¿æ’­ï¼‰
	SafeGoWithRestart("subscribeAllChannel", SSE.subscribeAllChannel, 3, 10*time.Second)
	// å¯åŠ¨å¿ƒè·³åç¨‹
	SafeGoWithRestart("heartbeat", SSE.heartbeat, 3, 10*time.Second)
	return SSE
}

// æ¸…ç†redisæœ¬èŠ‚ç‚¹çš„ä¼šè¯è®°å½•
func (s *Service) cleanupStaleSessions() {
	// è·å–æœ¬èŠ‚ç‚¹åœ¨ Redis ä¸­è®°å½•çš„æ‰€æœ‰å®¢æˆ·ç«¯ ID
	clients := s.rdb.SMembers(s.ctx, s.redisSessionSetKey).Val()

	// ä½¿ç”¨Pipelineæ‰¹é‡åˆ é™¤ä¼šè¯è®°å½•
	pipe := s.rdb.Pipeline()
	for _, clientId := range clients {
		sessionKey := s.redisSessionKey + clientId
		pipe.Del(s.ctx, sessionKey)
		// ä»ç”¨æˆ·å¯¹åº”çš„è®¾å¤‡Setä¸­ç§»é™¤è¯¥ clientId
		parts := strings.Split(clientId, ":")
		if len(parts) >= 2 { // ç¡®ä¿æ ¼å¼æ­£ç¡®ï¼Œå¦‚ "userId:uuid"
			userId := parts[0]
			userClientsKey := fmt.Sprintf("%s%s", s.redisUserClientsKey, userId)
			pipe.SRem(s.ctx, userClientsKey, clientId)
		}
	}

	// æœ€ååˆ é™¤æœ¬èŠ‚ç‚¹çš„é›†åˆ
	pipe.Del(s.ctx, s.redisSessionSetKey)

	// æ‰§è¡Œæ‰€æœ‰å‘½ä»¤
	_, err := pipe.Exec(s.ctx)
	if err != nil {
		log.Printf("Error cleaning up stale sessions: %v", err)
	}
}

// Connect SSEè¿æ¥
func (s *Service) Connect(c *gin.Context) {
	/*
		# SSEæ¥å£è¶…æ—¶æ—¶é—´ç‰¹æ®Šé…ç½®
		location /sse/ {
			proxy_pass http://your_gin_app_upstream;
			# ğŸ”¥ å…³é”®ï¼šä¸º SSE è®¾ç½®æé•¿çš„è¯»å–è¶…æ—¶
			proxy_read_timeout 3600s; # 1å°æ—¶ï¼Œæˆ–æ›´é•¿å¦‚ 86400s(24å°æ—¶)
			# åŒæ ·å»ºè®®ç¦ç”¨ç¼“å†²ï¼Œç¡®ä¿æ¶ˆæ¯å®æ—¶æ¨é€
			proxy_buffering off;
			proxy_cache off;
			# ... å…¶ä»–ä»£ç†è®¾ç½® ...
		}
	*/
	c.Header("Content-Type", "text/event-stream")
	c.Header("Cache-Control", "no-cache")
	c.Header("Connection", "keep-alive")
	c.Header("Access-Control-Allow-Origin", "*")

	userId := c.GetInt64("user_id")

	// åˆ›å»ºæ–°å®¢æˆ·ç«¯
	clientId := c.Query("client_id")
	if clientId == "" {
		c.AbortWithError(http.StatusInternalServerError, fmt.Errorf("client_id is empty"))
		return
	}
	// é¢„ç•™æ”¯æŒç”¨æˆ·å¤šç«¯ç™»å½•ï¼Œé€šè¿‡userid:uuidä½œä¸ºå®¢æˆ·ç«¯id
	clientId = fmt.Sprintf("%d:%s", userId, clientId)
	client := &Client{
		userId:      userId,
		clientId:    clientId,
		c:           c,
		messageChan: make(chan string, 100),
	}

	// æ³¨å†Œå®¢æˆ·ç«¯
	s.registerClient(clientId, client)

	// ä½¿ç”¨ Stream API æ›¿ä»£æ‰‹åŠ¨å¾ªç¯
	c.Stream(func(w io.Writer) bool {
		select {
		case msg, ok := <-client.messageChan:
			if !ok {
				return false // é€šé“å·²å…³é—­
			}
			// å‘é€äº‹ä»¶
			sse.Encode(w, sse.Event{
				Data: msg,
			})
			return true // ä¿æŒè¿æ¥
		case <-c.Request.Context().Done():
			// å®¢æˆ·ç«¯æ–­å¼€è¿æ¥
			s.unregisterClient(clientId)
			return false // æ–­å¼€è¿æ¥
		}
	})
}

// æ³¨å†Œå®¢æˆ·ç«¯
func (s *Service) registerClient(clientId string, client *Client) {
	// æœ¬åœ°æ³¨å†Œ
	s.clientsSync.Lock()
	s.clients[clientId] = client
	s.clientsSync.Unlock()

	// Redisæ³¨å†Œä¼šè¯ - ä½¿ç”¨Pipelineæ‰¹é‡æ“ä½œ
	pipe := s.rdb.Pipeline()
	sessionKey := s.redisSessionKey + clientId
	pipe.Set(s.ctx, sessionKey, s.nodeId, sessionTTL)
	pipe.SAdd(s.ctx, s.redisSessionSetKey, clientId)
	pipe.Expire(s.ctx, s.redisSessionSetKey, sessionTTL)

	// å°† clientId æ·»åŠ åˆ°ç”¨æˆ·å¯¹åº”çš„è®¾å¤‡Setä¸­
	userClientsKey := fmt.Sprintf("%s%d", s.redisUserClientsKey, client.userId)
	pipe.SAdd(s.ctx, userClientsKey, clientId)
	pipe.Expire(s.ctx, userClientsKey, sessionTTL) // ä¿æŒTTLä¸€è‡´

	_, err := pipe.Exec(s.ctx)
	if err != nil {
		log.Printf("Error registering client: %v", err)
	}

	// æ³¨å†ŒæˆåŠŸåï¼Œå¼‚æ­¥å‘é€ç¦»çº¿æ¶ˆæ¯
	SafeGoWithRestart("deliverOfflineMessages", func() {
		s.deliverOfflineMessages(client)
	}, 0, 0)
}

// deliverOfflineMessages å‘é€å¹¶æ¸…ç©ºç”¨æˆ·çš„ç¦»çº¿æ¶ˆæ¯
func (s *Service) deliverOfflineMessages(client *Client) {
	userOfflineQueueKey := fmt.Sprintf("%s:%d", s.redisOfflineQueueKey, client.userId)
	// å¾ªç¯å–å‡ºå¹¶åˆ é™¤åˆ—è¡¨ä¸­çš„æ‰€æœ‰æ¶ˆæ¯
	for {
		// ä½¿ç”¨RPOPä»åˆ—è¡¨å°¾éƒ¨å–å‡ºæ¶ˆæ¯ï¼ˆä¿è¯å…ˆè¿›å…ˆå‡ºï¼‰
		messageData, err := s.rdb.RPop(s.ctx, userOfflineQueueKey).Bytes()
		if errors.Is(err, redis.Nil) { // Redis.Nilè¡¨ç¤ºåˆ—è¡¨å·²ç©º
			break
		}
		if err != nil {
			log.Printf("Failed to get offline message for %d: %v", client.userId, err)
			break
		}

		// é‡æ–°æ ¼å¼åŒ–ä¸ºSSEæ ¼å¼å¹¶å‘é€ç»™å®¢æˆ·ç«¯
		formattedMsg := fmt.Sprintf("data: %s\n\n", string(messageData))
		client.messageChan <- formattedMsg
	}
	// æ‰€æœ‰ç¦»çº¿æ¶ˆæ¯å¤„ç†å®Œåï¼Œåˆ é™¤Keyï¼ˆå¯é€‰ï¼‰
	s.rdb.Del(s.ctx, userOfflineQueueKey)
}

// æ³¨é”€å®¢æˆ·ç«¯
func (s *Service) unregisterClient(clientId string) {
	// æœ¬åœ°æ³¨é”€
	s.clientsSync.Lock()
	if client, ok := s.clients[clientId]; ok {
		close(client.messageChan)
		delete(s.clients, clientId)
	}
	s.clientsSync.Unlock()

	// Redisæ³¨é”€ä¼šè¯ - ä½¿ç”¨Pipelineæ‰¹é‡æ“ä½œ
	pipe := s.rdb.Pipeline()
	sessionKey := s.redisSessionKey + clientId
	pipe.Del(s.ctx, sessionKey)
	pipe.SRem(s.ctx, s.redisSessionSetKey, clientId)

	// ä»ç”¨æˆ·å¯¹åº”çš„è®¾å¤‡Setä¸­ç§»é™¤è¯¥ clientId
	// æ³¨æ„ï¼šéœ€è¦å…ˆä» clientId ä¸­è§£æå‡º userId
	parts := strings.Split(clientId, ":")
	if len(parts) >= 2 { // ç¡®ä¿æ ¼å¼æ­£ç¡®ï¼Œå¦‚ "1:uuid"
		userId := parts[0]
		userClientsKey := fmt.Sprintf("%s%s", s.redisUserClientsKey, userId)
		s.rdb.SRem(s.ctx, userClientsKey, clientId)
	}

	_, err := pipe.Exec(s.ctx)
	if err != nil {
		log.Printf("Error unregistering client: %v", err)
	}
}

// å¿ƒè·³åç¨‹ï¼Œä¿æŒä¼šè¯æ´»è·ƒ
func (s *Service) heartbeat() {
	ticker := time.NewTicker(heartbeatInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			// è·å–æ‰€æœ‰å®¢æˆ·ç«¯ID
			s.clientsSync.RLock()
			clientIds := make([]string, 0, len(s.clients))
			for id := range s.clients {
				clientIds = append(clientIds, id)
			}
			s.clientsSync.RUnlock()

			if len(clientIds) == 0 {
				continue
			}

			// ä½¿ç”¨Pipelineæ‰¹é‡æ›´æ–°ä¼šè¯TTL
			pipe := s.rdb.Pipeline()
			for _, clientId := range clientIds {
				sessionKey := s.redisSessionKey + clientId
				pipe.Expire(s.ctx, sessionKey, sessionTTL)
				parts := strings.Split(clientId, ":")
				if len(parts) >= 2 { // ç¡®ä¿æ ¼å¼æ­£ç¡®ï¼Œå¦‚ "1:uuid"
					userId := parts[0]
					userClientsKey := fmt.Sprintf("%s%s", s.redisUserClientsKey, userId)
					pipe.Expire(s.ctx, userClientsKey, sessionTTL)
				}
			}
			pipe.Expire(s.ctx, s.redisSessionSetKey, sessionTTL)

			_, err := pipe.Exec(s.ctx)
			if err != nil {
				log.Printf("Error renewing sessions: %v", err)
			}
		case <-s.stopHeartbeat:
			return
		}
	}
}

// è®¢é˜…æœ¬èŠ‚ç‚¹ä¸“å±é¢‘é“
func (s *Service) subscribeNodeChannel() {
	pubsub := s.rdb.Subscribe(s.ctx, fmt.Sprintf("%s:%d", s.redisPubSubChannelKey, s.nodeId))
	defer pubsub.Close()

	for {
		_msg, err := pubsub.ReceiveMessage(s.ctx)
		if err != nil {
			fmt.Println("Error receiving message:", err)
			// å¤„ç†é”™è¯¯ï¼Œä¾‹å¦‚é‡æ–°è¿æ¥
			time.Sleep(1 * time.Second)
			continue
		}

		s.subscribeChannel(_msg)
	}
}

// è®¢é˜…å…¨å±€æ§åˆ¶é¢‘é“ï¼ˆç”¨äºå¹¿æ’­ï¼‰
func (s *Service) subscribeAllChannel() {
	pubsub := s.rdb.Subscribe(s.ctx, s.redisPubSubChannelKey)
	defer pubsub.Close()

	for {
		_msg, err := pubsub.ReceiveMessage(s.ctx)
		if err != nil {
			fmt.Println("Error receiving message:", err)
			// å¤„ç†é”™è¯¯ï¼Œä¾‹å¦‚é‡æ–°è¿æ¥
			time.Sleep(1 * time.Second)
			continue
		}

		s.subscribeChannel(_msg)
	}
}

func (s *Service) subscribeChannel(_msg *redis.Message) {
	var msg Message
	if err := json.Unmarshal([]byte(_msg.Payload), &msg); err != nil {
		fmt.Println("Error unmarshalling message:", err)
		return
	}
	// ç›®æ ‡å®¢æˆ·ç«¯Idä¸ºç©ºè¯´æ˜æ˜¯ç¾¤å‘æ¶ˆæ¯
	if msg.ClientId == "" {
		for k, _ := range s.clients {
			msg.ClientId = k
			s.deliverToClient(&msg)
		}
	} else {
		// åªå¤„ç†ç›®æ ‡ä¸ºæœ¬èŠ‚ç‚¹çš„æ¶ˆæ¯
		if msg.NodeId == s.nodeId {
			s.deliverToClient(&msg)
		}
	}
}

// å‘æŒ‡å®šå®¢æˆ·ç«¯å‘é€æ¶ˆæ¯ï¼ˆé›†ç¾¤æ„ŸçŸ¥ï¼‰
func (s *Service) sendToClient(msg *Message) bool {
	// æŸ¥æ‰¾å®¢æˆ·ç«¯æ‰€åœ¨èŠ‚ç‚¹
	sessionKey := s.redisSessionKey + msg.ClientId
	var nodeId = -1 // èŠ‚ç‚¹idä»0å¼€å§‹ï¼Œè®¾ç½®-1é˜²æ­¢å’Œç©ºå€¼æ··æ·†
	var err error
	nodeId, err = s.rdb.Get(s.ctx, sessionKey).Int()
	if err != nil || nodeId == -1 {
		// å®¢æˆ·ç«¯ä¸åœ¨çº¿
		return false
	}

	msg.NodeId = nodeId
	msg.Timestamp = time.Now().Unix()

	if nodeId == s.nodeId {
		// å®¢æˆ·ç«¯åœ¨æœ¬èŠ‚ç‚¹ï¼Œç›´æ¥å‘é€
		s.deliverToClient(msg)
		return true
	}

	// å‘é€åˆ°ç›®æ ‡èŠ‚ç‚¹
	msgData, _ := json.Marshal(msg)
	s.rdb.Publish(s.ctx, fmt.Sprintf("%s:%d", s.redisPubSubChannelKey, nodeId), string(msgData))
	return true
}

// SendToUser å‘æŒ‡å®šç”¨æˆ·çš„æ‰€æœ‰åœ¨çº¿è®¾å¤‡å‘é€æ¶ˆæ¯ï¼ˆé›†ç¾¤æ„ŸçŸ¥ï¼‰
func (s *Service) SendToUser(userId int64, msgData any, msgType int) {
	// 1. æ„é€ æ¶ˆæ¯ä½“
	msg := &Message{
		// ClientId ç•™ç©ºï¼Œå› ä¸ºç›®æ ‡æ˜¯ç”¨æˆ·ï¼Œä¸æ˜¯ç‰¹å®šè®¾å¤‡
		Type:      msgType,
		Data:      msgData,
		Timestamp: time.Now().Unix(),
	}

	// 2. è·å–è¯¥ç”¨æˆ·æ‰€æœ‰çš„ clientId
	userClientsKey := fmt.Sprintf("%s%d", s.redisUserClientsKey, userId)
	clientIds, err := s.rdb.SMembers(s.ctx, userClientsKey).Result()
	if err != nil {
		log.Printf("Error getting client list for user %d: %v", userId, err)
		return
	}

	if len(clientIds) == 0 {
		// ç”¨æˆ·æ‰€æœ‰è®¾å¤‡éƒ½ä¸åœ¨çº¿ï¼Œå¯ä»¥é€‰æ‹©å­˜å…¥ç¦»çº¿æ¶ˆæ¯
		data, err := json.Marshal(msg)
		if err != nil {
			fmt.Println("json.Marshal err:", err)
			return
		}
		s.storeOfflineMessage(strconv.FormatInt(userId, 10), data)
		return
	}

	// 3. éå†æ‰€æœ‰ clientIdï¼Œå‘é€æ¶ˆæ¯
	for _, targetClientId := range clientIds {
		// ä¸ºæ¯ä¸ªç›®æ ‡è®¾å¤‡æ„é€ ä¸€ä¸ªç›®æ ‡æ˜ç¡®çš„æ¶ˆæ¯
		targetMsg := &Message{
			ClientId:  targetClientId, // è¿™é‡ŒæŒ‡å®šå…·ä½“çš„è®¾å¤‡
			NodeId:    msg.NodeId,
			Type:      msg.Type,
			Data:      msg.Data,
			Timestamp: msg.Timestamp,
		}
		// è°ƒç”¨å•æ’­å‘é€é€»è¾‘ï¼Œè¿™ä¼šå¤„ç†èŠ‚ç‚¹è·¯ç”±
		s.sendToClient(targetMsg)
	}
}

// BroadcastMessage å‘æ‰€æœ‰å®¢æˆ·ç«¯å¹¿æ’­æ¶ˆæ¯
func (s *Service) BroadcastMessage(msgData any, msgType int) {
	// 1. æ„é€ æ¶ˆæ¯ä½“
	msg := &Message{
		Type:      msgType,
		Data:      msgData,
		Timestamp: time.Now().Unix(),
	}
	data, _ := json.Marshal(msg)
	// ç»™æ‰€æœ‰èŠ‚ç‚¹æ¨é€
	s.rdb.Publish(s.ctx, s.redisPubSubChannelKey, string(data))
}

// å‘é€æ¶ˆæ¯åˆ°æŒ‡å®šå®¢æˆ·ç«¯
func (s *Service) deliverToClient(msg *Message) {
	s.clientsSync.RLock()
	defer s.clientsSync.RUnlock()

	data, err := json.Marshal(msg)
	if err != nil {
		fmt.Println("json.Marshal err:", err)
		return
	}

	if client, ok := s.clients[msg.ClientId]; ok {
		msg := fmt.Sprintf("data: %s\n\n", data)
		client.messageChan <- msg
	} else {
		// ç”¨æˆ·ä¸åœ¨çº¿åˆ™ä¿å­˜åˆ°ç¦»çº¿æ¶ˆæ¯
		userClientId := strings.Split(msg.ClientId, ":")
		if len(userClientId) == 2 {
			s.storeOfflineMessage(userClientId[0], data)
		}
	}
}

// storeOfflineMessage å°†æ¶ˆæ¯å­˜å…¥ç”¨æˆ·çš„ç¦»çº¿é˜Ÿåˆ— (Redis List)
func (s *Service) storeOfflineMessage(userId string, messageData []byte) bool {
	// ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„List
	userOfflineQueueKey := fmt.Sprintf("%s:%s", s.redisOfflineQueueKey, userId)
	// ä½¿ç”¨LPUSHå°†æ¶ˆæ¯å­˜å…¥åˆ—è¡¨å¤´éƒ¨ï¼Œå¹¶è®¾ç½®æ•´ä¸ªKeyçš„TTL
	err := s.rdb.LPush(s.ctx, userOfflineQueueKey, messageData).Err()
	if err != nil {
		log.Printf("Failed to store offline message for %s: %v", userId, err)
		return false
	}
	return true
}

// Close å…³é—­æœåŠ¡
func (s *Service) Close() {
	close(s.stopHeartbeat)

	// æ¸…ç†æ‰€æœ‰æœ¬åœ°å®¢æˆ·ç«¯
	s.clientsSync.Lock()
	for clientId, client := range s.clients {
		close(client.messageChan)
		delete(s.clients, clientId)
	}
	s.clientsSync.Unlock()

	// æ¸…ç†Redisä¸­çš„æœ¬èŠ‚ç‚¹ä¼šè¯
	s.cleanupStaleSessions()
}

// SafeGoWithRestart å®‰å…¨åœ°å¯åŠ¨ä¸€ä¸ªåç¨‹ï¼Œå¹¶åœ¨panicåå»¶è¿Ÿè‡ªåŠ¨é‡å¯ï¼ˆå¸¦æ¬¡æ•°é™åˆ¶ï¼‰
// goroutineName: åç¨‹åç§°
// f: è¦æ‰§è¡Œçš„å‡½æ•°
// maxRestarts: æœ€å¤§é‡å¯æ¬¡æ•°ï¼Œé˜²æ­¢æ— é™é‡å¯è€—å°½èµ„æº
// restartDelay: é‡å¯å»¶è¿Ÿæ—¶é—´ï¼Œé¿å…ç«‹å³é‡å¯å¯èƒ½åŠ å‰§é—®é¢˜
func SafeGoWithRestart(goroutineName string, f func(), maxRestarts int, restartDelay time.Duration) {
	restarts := 0
	var run func()
	run = func() {
		defer func() {
			if r := recover(); r != nil {
				log.Printf("PANIC recovered in goroutine [%s]: %v\nStack Trace:\n%s. Restarts left: %d",
					goroutineName, r, string(debug.Stack()), maxRestarts-restarts)
				if restarts < maxRestarts {
					restarts++
					time.Sleep(restartDelay) // å»¶è¿Ÿé‡å¯
					go run()                 // é‡å¯åç¨‹
				} else {
					log.Printf("CRITICAL: Goroutine [%s] reached max restarts, exiting.", goroutineName)
				}
			}
		}()
		f()
	}
	go run()
}
